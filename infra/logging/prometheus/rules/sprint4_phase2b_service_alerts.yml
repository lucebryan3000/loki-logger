groups:
  - name: sprint4_phase2b_service_alerts_v1
    interval: 30s
    rules:
      # Service-specific alert rules for actionable errors identified in Phase 2B
      # These complement generic infrastructure alerts with service-specific visibility

      # --- Critical Service Failures ---
      - alert: MCPServerRepeatedFailures
        expr: >
          sum(increase(loki_write_dropped_entries_total[10m])) > 0
          or (count(count_over_time({service_name="mcp-server.service", level="error"}[5m])) > 10)
        for: 2m
        labels:
          severity: critical
          service: mcp-server
          phase: 2b
        annotations:
          runbook_url: "docs/troubleshooting.md#mcp-server-failures"
          summary: "MCP Server repeated failures detected"
          description: "mcp-server.service has shown {{ $value }} failure events in the last 5 minutes (threshold: >10)"

      - alert: E2ECheckServiceFailure
        expr: count(count_over_time({service_name="logging-e2e-check.service", level="error"}[5m])) > 3
        for: 5m
        labels:
          severity: warning
          service: logging-e2e-check
          phase: 2b
        annotations:
          runbook_url: "docs/operations.md#e2e-checks"
          summary: "Logging E2E check service failure"
          description: "logging-e2e-check.service has {{ $value }} errors (threshold: >3). Monitoring validation may be impacted."

      # --- Security Event Monitoring ---
      - alert: UnusualUFWBlockActivity
        expr: >
          sum(rate(loki_write_failures_discarded_total[5m])) > 0
          or (sum(count_over_time({service_name="ufw", level="error"}[5m])) > 100)
        for: 5m
        labels:
          severity: info
          service: ufw
          category: security
          phase: 2b
        annotations:
          runbook_url: "docs/security.md#firewall-events"
          summary: "Unusual firewall (UFW) block activity"
          description: "UFW has blocked {{ $value }} connections in the last 5 minutes"

      # --- Unknown Service Error Spike ---
      - alert: UnknownServiceErrorSpike
        expr: >
          (sum(count_over_time({service_name="unknown_service", level="error"}[5m]))
           / (sum(count_over_time({service_name="unknown_service"}[30m])) / 6 + 1)) > 2
        for: 3m
        labels:
          severity: info
          service: unknown_service
          phase: 2b
        annotations:
          runbook_url: "docs/troubleshooting.md#unknown-service-errors"
          summary: "Error rate spike in unknown_service entries"
          description: "unknown_service error rate has more than doubled in the last 5 minutes. Investigate service_name extraction coverage."

      # --- Recording Rules for Dashboard Queries ---
      - record: sprint4:mcp_server_errors:rate5m
        expr: rate(count_over_time({service_name="mcp-server.service", level="error"}[5m])[5m:1m])

      - record: sprint4:e2e_check_errors:rate5m
        expr: rate(count_over_time({service_name="logging-e2e-check.service", level="error"}[5m])[5m:1m])

      - record: sprint4:security_events:total_5m
        expr: sum(count_over_time({service_name=~"ufw|sudo|sshd|cron"}[5m]))

      - record: sprint4:unknown_service_error_ratio:5m
        expr: >
          sum(count_over_time({service_name="unknown_service", level="error"}[5m]))
          / (sum(count_over_time({service_name="unknown_service"}[5m])) + 1)

      # Note: Phase 2C metrics moved to Phase 3A/3B groups to avoid aggregation conflicts

  # --- Phase 3A Service Health Recording Rules ---
  - name: sprint4_phase3a_service_health_v1
    interval: 60s
    rules:
      # Service availability (based on log activity in last hour)
      - record: sprint4:service_log_count:1h
        expr: >
          sum by (service_name) (count_over_time({log_source="rsyslog_syslog"}[1h]))

      # Error-to-log ratio (health indicator)
      - record: sprint4:service_error_ratio:1h
        expr: >
          sum by (service_name) (
            count_over_time({level="error"}[1h])
          ) / (sum by (service_name) (count_over_time({log_source="rsyslog_syslog"}[1h])) + 1)

      # Critical service error count
      - record: sprint4:critical_service_errors:1h
        expr: >
          sum by (service_name) (
            count_over_time({service_name=~"loki|prometheus|grafana|alloy", level="error"}[1h])
          )

      # High error services (more than 10 errors per hour)
      - record: sprint4:high_error_services:1h
        expr: >
          sum by (service_name) (count_over_time({level="error"}[1h])) > 10

      # Total infrastructure error count
      - record: sprint4:infra_error_count:5m
        expr: >
          sum(count_over_time({level="error"}[5m]))

  # --- Phase 3B Service Availability & Health Metrics ---
  - name: sprint4_phase3b_service_availability_v1
    interval: 60s
    rules:
      # Service availability indicator (based on recent log activity)
      - record: sprint4:service_is_active:5m
        expr: >
          count by (service_name) (count_over_time({log_source="rsyslog_syslog"}[5m]) > 0)

      # Service uptime percentage (active hours vs total hours)
      - record: sprint4:service_availability_pct:1h
        expr: >
          (count by (service_name) (count_over_time({log_source="rsyslog_syslog"}[1h]) > 0) / count by (service_name) (vector(1)) * 100)

      # Service error-free indicator (no errors in window)
      - record: sprint4:service_error_free:1h
        expr: >
          (count_over_time({log_source="rsyslog_syslog"}[1h]) > 0) unless (sum by (service_name) (count_over_time({level="error"}[1h])) > 0)

      # Service health score (composite: active + low error rate)
      - record: sprint4:service_health_score:1h
        expr: >
          (count by (service_name) (count_over_time({log_source="rsyslog_syslog"}[1h]) > 0)
          * (1 - (sum by (service_name) (count_over_time({level="error"}[1h])) / (sum by (service_name) (count_over_time({log_source="rsyslog_syslog"}[1h])) + 1))))

      # Critical service uptime (infrastructure services only)
      - record: sprint4:critical_service_uptime:1h
        expr: >
          count by (service_name) (count_over_time({service_name=~"loki|prometheus|grafana|alloy", log_source="rsyslog_syslog"}[1h]) > 0)

      # Service log volume trend (for detecting quiet services)
      - record: sprint4:service_log_volume_per_hour:1h
        expr: >
          sum by (service_name) (count_over_time({log_source="rsyslog_syslog"}[1h]))

      # Service error trend (for detecting error spikes)
      - record: sprint4:service_error_count_1h:1h
        expr: >
          sum by (service_name) (count_over_time({level="error"}[1h]))

      # Stable service indicator (no errors, consistent volume)
      - record: sprint4:stable_services:1h
        expr: >
          (sum by (service_name) (count_over_time({log_source="rsyslog_syslog"}[1h]) > 0))
          unless (sum by (service_name) (count_over_time({level="error"}[1h])) > 0)

  # --- Phase 3C Log Pipeline Health Metrics ---
  # Note: Pipeline health monitoring is tracked via Loki queries in dashboards
  # Prometheus can track write_dropped/write_failures from Loki exporter if available
